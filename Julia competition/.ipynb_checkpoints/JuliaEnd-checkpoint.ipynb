{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using gpu device 0: GeForce GTX 750 (CNMeM is enabled with initial size: 60.0% of memory, cuDNN 5005)\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import theano\n",
    "import theano.tensor as T\n",
    "import lasagne\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np\n",
    "\n",
    "def read_data(typeData, labelsInfo, imageSize, path):\n",
    "    #Intialize x  matrix\n",
    "#     x = np.zeros((labelsInfo.shape[0], imageSize))\n",
    "    x = []\n",
    "\n",
    "    for (index, idImage) in enumerate(labelsInfo[\"ID\"]):\n",
    "        #Read image file\n",
    "        nameFile = \"{0}/{1}Resized/{2}.Bmp\".format(path, typeData, idImage)\n",
    "        img = cv2.imread(nameFile, 0)\n",
    "\n",
    "#         x[index, :] = np.reshape(img, (1, imageSize))\n",
    "        img = img.reshape((20,20))\n",
    "\n",
    "        x.append(img)\n",
    "#         x.append(img / np.float32(256))\n",
    "    return np.array(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Set location of data files , folders\n",
    "path = '.'\n",
    "imageSize = 400\n",
    "\n",
    "labelsInfoTrain = pd.read_csv(\"{0}/trainLabels.csv\".format(path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "xTrain = read_data(\"train\", labelsInfoTrain, imageSize, path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def rotation_img(img_arr):\n",
    "    img_res = []\n",
    "    def rotate(img):\n",
    "        new = []\n",
    "        for i in range(-15,20,5):\n",
    "            rows,cols = img.shape\n",
    "            if i == 0:\n",
    "                M = cv2.getRotationMatrix2D((cols/2,rows/2),i,1)\n",
    "            else:\n",
    "                M = cv2.getRotationMatrix2D((cols/2,rows/2),i,1.2)\n",
    "            dst = cv2.warpAffine(img,M,(cols,rows))\n",
    "            new.append([dst])\n",
    "        return np.array(new)\n",
    "        \n",
    "    for i in range(img_arr.shape[0]):\n",
    "        changed = rotate(img_arr[i])\n",
    "        img_res.append(changed)\n",
    "    \n",
    "    return np.concatenate(img_res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "xTrain = rotation_img(xTrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "xTrain = xTrain / np.float32(256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Read information about test data ( IDs ).\n",
    "labelsInfoTest = pd.read_csv(\"{0}/sampleSubmission.csv\".format(path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "new_lab = []\n",
    "yTrain = map(ord, labelsInfoTrain[\"Class\"])\n",
    "for char in yTrain:\n",
    "    new_lab.append([char for _ in range(7)])\n",
    "yTrain = np.concatenate(new_lab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "all_labels = sorted(list(set(yTrain)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "labels = np.array(map(lambda x: all_labels.index(x), yTrain)).astype(np.uint8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "val_indexes = np.random.randint(0,len(labels),8800)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_indexes = list(set(range(len(labels))) - set(val_indexes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train = xTrain\n",
    "y_train = labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_val = xTrain[val_indexes]\n",
    "y_val = labels[val_indexes]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def build_cnn(input_var=None):\n",
    "    network = lasagne.layers.InputLayer(shape=(None, 1, 20, 20),\n",
    "                                        input_var=input_var)\n",
    "    \n",
    "#     network = lasagne.layers.batch_norm(network)\n",
    "#     network = lasagne.layers.dropout(network, p=0.8)\n",
    "# #     network = lasagne.layers.batch_norm(lasagne.layers.Conv2DLayer(\n",
    "# #             network, num_filters=128, filter_size=(3, 3),\n",
    "# #             nonlinearity=lasagne.nonlinearities.LeakyRectify(),\n",
    "# #             W=lasagne.init.GlorotUniform()))\n",
    "    \n",
    "    network = lasagne.layers.Conv2DLayer(\n",
    "            network, num_filters=128, filter_size=(3, 3),\n",
    "            nonlinearity=lasagne.nonlinearities.LeakyRectify(),\n",
    "            W=lasagne.init.GlorotUniform())\n",
    "    \n",
    "    network = lasagne.layers.Conv2DLayer(\n",
    "            network, num_filters=128, filter_size=(3, 3),\n",
    "            nonlinearity=lasagne.nonlinearities.LeakyRectify(),\n",
    "            W=lasagne.init.GlorotUniform())\n",
    "\n",
    "    network = lasagne.layers.MaxPool2DLayer(network, pool_size=(2, 2))\n",
    "    \n",
    "    network = lasagne.layers.Conv2DLayer(\n",
    "            network, num_filters=256, filter_size=(3, 3),\n",
    "            nonlinearity=lasagne.nonlinearities.LeakyRectify(),\n",
    "            W=lasagne.init.GlorotUniform())\n",
    "    \n",
    "    network = lasagne.layers.MaxPool2DLayer(network, pool_size=(2, 2))\n",
    "\n",
    "    network = lasagne.layers.DenseLayer(\n",
    "            lasagne.layers.dropout(network, p=.5),\n",
    "            num_units=1024,\n",
    "            nonlinearity=lasagne.nonlinearities.LeakyRectify())\n",
    "\n",
    "    network = lasagne.layers.DenseLayer(\n",
    "            lasagne.layers.dropout(network, p=.5),\n",
    "            num_units=62,\n",
    "            nonlinearity=lasagne.nonlinearities.softmax)\n",
    "\n",
    "    return network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def iterate_minibatches(inputs, targets, batchsize, shuffle=False):\n",
    "    assert len(inputs) == len(targets)\n",
    "    if shuffle:\n",
    "        indices = np.arange(len(inputs))\n",
    "        np.random.shuffle(indices)\n",
    "    for start_idx in range(0, len(inputs) - batchsize + 1, batchsize):\n",
    "        if shuffle:\n",
    "            excerpt = indices[start_idx:start_idx + batchsize]\n",
    "        else:\n",
    "            excerpt = slice(start_idx, start_idx + batchsize)\n",
    "        yield inputs[excerpt], targets[excerpt]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "input_var = T.tensor4('inputs')\n",
    "target_var = T.ivector('targets')\n",
    "# lr = theano.shared(np.array(0.1, dtype=theano.shared.floatX))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "network = build_cnn(input_var)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "prediction = lasagne.layers.get_output(network)\n",
    "loss = lasagne.objectives.categorical_crossentropy(prediction, target_var)\n",
    "loss = loss.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "params = lasagne.layers.get_all_params(network, trainable=True)\n",
    "# updates = lasagne.updates.nesterov_momentum(\n",
    "#             loss, params, learning_rate=0.01, momentum=0.9)\n",
    "\n",
    "updates = lasagne.updates.adamax(loss, params, learning_rate=0.005)\n",
    "\n",
    "test_prediction = lasagne.layers.get_output(network, deterministic=True)\n",
    "test_loss = lasagne.objectives.categorical_crossentropy(test_prediction,\n",
    "                                                            target_var)\n",
    "test_loss = test_loss.mean()\n",
    "\n",
    "test_acc = T.mean(T.eq(T.argmax(test_prediction, axis=1), target_var),\n",
    "                        dtype=theano.config.floatX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pred = test_prediction.argmax(-1)\n",
    "f_predict = theano.function([input_var], pred,\n",
    "                            allow_input_downcast=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_fn = theano.function([input_var, target_var], \n",
    "                           loss, \n",
    "                           updates=updates,\n",
    "                           allow_input_downcast=True)\n",
    "\n",
    "val_fn = theano.function([input_var, target_var], \n",
    "                         [test_loss, test_acc],\n",
    "                         allow_input_downcast=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "num_epochs = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "BATCH_SIZE = 512"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, train:0.0918484444566\n",
      "Epoch: 1, train:0.0957874259966\n",
      "Epoch: 2, train:0.0919433176517\n",
      "Epoch: 3, train:0.0924889521125\n",
      "Epoch: 4, train:0.0835439541761\n",
      "Epoch: 5, train:0.0813113454948\n",
      "Epoch: 6, train:0.0850154898622\n",
      "Epoch: 7, train:0.0800750095616\n",
      "Epoch: 8, train:0.0827560438391\n",
      "Epoch: 9, train:0.0816941253841\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(num_epochs):\n",
    "\n",
    "    train_err = 0\n",
    "    train_batches = 0\n",
    "    for batch in iterate_minibatches(X_train, y_train, BATCH_SIZE, shuffle=True):\n",
    "        inputs, targets = batch\n",
    "        train_err += train_fn(inputs, targets)\n",
    "        train_batches += 1\n",
    "\n",
    "    And a full pass over the validation data:\n",
    "    val_err = 0\n",
    "    val_acc = 0\n",
    "    val_batches = 0\n",
    "    for batch in iterate_minibatches(X_val, y_val, BATCH_SIZE, shuffle=False):\n",
    "        inputs, targets = batch\n",
    "        err, acc = val_fn(inputs, targets)\n",
    "        val_err += err\n",
    "        val_acc += acc\n",
    "        val_batches += 1\n",
    "    print(\"Epoch: {}, train:{}\".format(epoch, val_err / val_batches,\n",
    "                                               train_err/train_batches))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'y_val' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-33-c4d9df7c7cd5>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m8800\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m     \u001b[0my_true\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_val\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m     \u001b[0my_pred\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf_predict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mX_val\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'y_val' is not defined"
     ]
    }
   ],
   "source": [
    "y_true = []\n",
    "y_pred = []\n",
    "\n",
    "for i in range(8800):\n",
    "    y_true.append(int(y_val[i]))\n",
    "    y_pred.append(f_predict(np.array([X_val[i]]))[0])\n",
    "\n",
    "print np.equal(y_true, y_pred).mean()\n",
    "CM = confusion_matrix(y_true, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "fig = plt.figure(figsize=(20, 20))\n",
    "sns.heatmap(CM, annot=True, fmt=\"d\", linewidths=.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def read_data(typeData, labelsInfo, imageSize, path):\n",
    "    #Intialize x  matrix\n",
    "#     x = np.zeros((labelsInfo.shape[0], imageSize))\n",
    "    x = []\n",
    "\n",
    "    for (index, idImage) in enumerate(labelsInfo[\"ID\"]):\n",
    "        #Read image file\n",
    "        nameFile = \"{0}/{1}Resized/{2}.Bmp\".format(path, typeData, idImage)\n",
    "        img = cv2.imread(nameFile, 0)\n",
    "\n",
    "#         x[index, :] = np.reshape(img, (1, imageSize))\n",
    "        img = img.reshape((1,20,20))\n",
    "\n",
    "#         x.append(img)\n",
    "        x.append(img / np.float32(256))\n",
    "    return np.array(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "xTest = read_data(\"test\", labelsInfoTest, imageSize, path)\n",
    "\n",
    "y_test_pred = []\n",
    "for i in range(xTest.shape[0]):\n",
    "    y_test_pred.append(f_predict(np.array([xTest[i]]))[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "y_test_pred = np.array(y_test_pred)\n",
    "# np.savetxt('submit.txt', y_test_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "first = np.arange(48,58)\n",
    "second = np.arange(65,91)\n",
    "third = np.arange(97,123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "f = np.array(range(10))\n",
    "s = np.array(range(10,36))\n",
    "th = np.array(range(36, 62))\n",
    "chars = []\n",
    "for i in y_test_pred:\n",
    "\n",
    "    if i in f:\n",
    "        chars.append(chr(48 + i))\n",
    "    elif i in s:\n",
    "        chars.append(chr(55 + i))\n",
    "    else:\n",
    "        chars.append(chr(61 + i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for i in range(len(chars)):\n",
    "    labelsInfoTest.set_value(i,'Class', chars[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "labelsInfoTest.to_csv('sampleSubmission.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
